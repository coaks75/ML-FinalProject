{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Neural Network in TensorFlow\n",
    "\n",
    "In this notebook, we build a 2-hidden layers neural network (a.k.a multilayer perceptron) with TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Overview\n",
    "\n",
    "This example uses the MNIST handwritten digits dataset. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "## Creating the Model\n",
    "\n",
    "The task of our network is to look at the handwritten digits and determine which nubmer it is 0-9. We will formulate this as a 10-class classification problem, where the output will be the probability of the handwritten image belonging to each class. \n",
    "\n",
    "First let's set up our environment and load the data. The MNIST data is hosted on Yann LeCun's website. For your convenience, we've included some python code to download and install the data automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "mnist = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# Data Parameters\n",
    "num_features = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1    # alpha for gradient descent\n",
    "num_steps = 2000       # iterations for gradient descent\n",
    "batch_size = 256       # number of inputs to look at simultaneously (good for large data!)\n",
    "display_step = 100     # when to print out some feedback\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "\n",
    "# Divide the data into training and testing\n",
    "(x_train, y_train), (x_test, y_test) = mnist\n",
    "\n",
    "print(x_train.shape)\n",
    "# Some data preprocessing to make this go more smoothly\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "print(x_train.shape)\n",
    "# Flatten images to 1-D vector of 784 features (28*28 pixels).\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "# Use tf.data API to shuffle and batch data (batches will make it faster to queue up \n",
    "# sets of images for training all at one time--a convenient way to split up large data sets)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Structure\n",
    "Now we can define our neural network model. To do this, we define a new class for our model that inherits from the built-in TensorFlow `Model` class. Then, we just specify each layer of the nework; we only need to indicate how many nodes will be at each layer and what the activation function looks like, and do not have to explicitly list any of the weights. We are going to use the rectified linear function as our activation function (threshold function) for the perceptrons at layers 1 and 2. \n",
    "\n",
    "Because this is a multi-class classification problem (which class 0-9 does the handwritten digit fall into?), we are going to use the same softmax normalization we discussed in class (and which can be found in the `NeuralNetwork-demo.ipynb` eample). Rather than just making a yes/no decision about each class and only choosing one, softmax lets us output a probability for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Model. Our NeuralNet inherits from the generic TF Model class\n",
    "class NeuralNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully-connected hidden layer. Activation function (threshold function) is \n",
    "        # rectified linear.\n",
    "        self.h1 = layers.Dense(n_hidden_1, activation=tf.nn.relu)\n",
    "        # First fully-connected hidden layer. Activation function (threshold function) is \n",
    "        # rectified linear.\n",
    "        self.h2 = layers.Dense(n_hidden_2, activation=tf.nn.relu)\n",
    "        # Second fully-connecter hidden layer. Activation function is using \"softmax\" to\n",
    "        # normalize output as a probability distribution over the 10 classes (digits 0-9)\n",
    "        self.out = layers.Dense(num_classes, activation=tf.nn.softmax)\n",
    "\n",
    "    # Set forward pass--this defines the input layer (h1) and output layer (out) and then\n",
    "    # formats the final results as a probability distribution over the classes using softmax\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.h1(x)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits (positions on the logistic regression sigmoid)\n",
    "            # without softmax normalization, so only apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build neural network model.\n",
    "neural_net = NeuralNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the Loss Function\n",
    "\n",
    "The next step is to define our loss function, which we will need for training. Again, because we are doing a probabilistic multi-class classification, we will use the cross-entropy loss function presented in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits as part of the function, so don't do\n",
    "# it before calling.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric. This counts how many of our predictions we get right based on \n",
    "# choosing the prediction with the highest probability. \n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Next, we need to write our training loop. TensorFlow has many different optimization functions you can use to minimize loss with respect to the parameters. However, we are going to stick with gradient descent. Specifically, we will be using a variation of gradient descent called ***stochastic gradient descent***. \n",
    "\n",
    "Standard gradient descent--typically called batch gradient descent because we process the data in one big batch--will look at each point in the training data in sequence, which gives us a good model, but can be impossible with large datasets that maybe don't all fit in memory. Stochastic gradient descent (SGD) divides the training data into managably sized batches, runs a mini-gradient descent on all of those, and averages the results to get the final answer. It still looks at all the data, but in more \"bite-sized\" pieces. The results of SGD are usually pretty close to the true gradient descent, and it is much more tractable for large datasets (and often less suceptible to getting stuck in local minima). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation \n",
    "    # (see Backprop.ipnb for an explanation of the GradientTape)\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = neural_net(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = neural_net.trainable_variables\n",
    "\n",
    "    # Compute gradients (backpropagation).\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update all of the weights W and biases (y-intercepts) b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(num_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x, is_training=True)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"Pred:\", pred)\n",
    "        print(\"Actual:\", batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "We now have a trained neural network ready for testing! Let's run it on the testing data and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on validation set.\n",
    "pred = neural_net(x_test, is_training=False)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea how the model is performing, we can print out some of the handwritten letters in the validation data and the prediction made by our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize predictions.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict 5 images from validation set.\n",
    "n_images = 20\n",
    "test_images = x_test[:n_images]\n",
    "predictions = neural_net(test_images)\n",
    "\n",
    "# Display image and model prediction.\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
