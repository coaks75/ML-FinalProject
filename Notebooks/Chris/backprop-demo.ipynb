{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Examples\n",
    "\n",
    "In this notebook will look at three different ways of computing gradients for neural network learning, all of which rely on the same basic principle (backprop). \n",
    "\n",
    "1. The first method is to code a function and its derivatives manually using the rules of backprop.\n",
    "2. The second method is to use automatic differentiation using a python tool called [autograd](https://github.com/HIPS/autograd).\n",
    "3. The third method is to use [Tensorflow](https://www.tensorflow.org/), a powerful machine learning toolkit from Google, which also includes methods to automatically differentiate an expression defined through Tensforflow primitives. \n",
    "\n",
    "### More about ``autograd``\n",
    "\n",
    "[autograd](https://github.com/HIPS/autograd) is a Python package for **algorithmic differentiation**. It allows you to automatically compute the derivative of functions written in (nearly) native code. This makes it really easy to compute derivatives for things like gradients of complex non-linear functions in neural networks. Under the hood, it is also using reverse mode autodiff (which is just backpropagation). To use autograd, you just need to add it to your cs490 Anaconda environment using the following command:\n",
    "\n",
    "`conda install -c conda-forge autograd`\n",
    "\n",
    "### More about Tensorflow\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/) is a powerful machine learning package from Google.\n",
    "\n",
    "You can define functions as \"computation graphs\" using tensor flow operations, and it can automatically perform backprop (aka reverse mode automatic differentiation) to compute the gradient for you.\n",
    "\n",
    "You can use the Python port of Tensorflow in the same way as autograd--you just need to download it and install it in your Anaconda cs490 environment. To do this, use the following command:\n",
    "\n",
    "`conda install tensorflow`\n",
    "\n",
    "If you have a GPU available on your machine, you can download the GPU compatible version of Tensorflow. This is optimized to run different parts of the \"tensor\" (essentially a matrix on steroids, where each cell can be any type of data...like images...which are just matrices of pixels) on different GPU cores and will be much more efficient if you are training a deep network or have a lot of training data. To install the GPU version, use this command: \n",
    "\n",
    "`conda install tensorflow-gpu` \n",
    "\n",
    "**NOTE: the GPU version is apparently not available for Mac**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Example 1\n",
    "\n",
    "We will see three different ways to compute the gradient of\n",
    "\n",
    "$$f(x,y,z) = (2x + y)*z$$\n",
    "\n",
    "where we can interpret this as a very simple two node network. $2x + y$ is one perceptron at the first layer of our network (i.e., the hidden layer) where we are computing a linear combination over the input $x$. The result of this is used by the next layer (i.e., the output layer) to compute the final output with $z$.\n",
    "\n",
    "### Manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "[6. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Backprop example\n",
    "\n",
    "# Compute f(x,y,z) = (2*x+y)*z\n",
    "x = 1.\n",
    "y = 2.\n",
    "z = 3.\n",
    "\n",
    "# Forward pass--push input through the network to get the predition f\n",
    "h = 2.*x + y   # Node 1\n",
    "f = h*z        # Node 2\n",
    "\n",
    "# Backward pass--get the derivative with respect to each component to get the gradient\n",
    "d_f = 1\n",
    "d_h = z * d_f  # Node 2 input\n",
    "d_z = h * d_f  # Node 2 input\n",
    "d_x = 2 * d_h  # Node 1 input\n",
    "d_y = 1 * d_h  # Node 1 input\n",
    "\n",
    "grad = np.array([d_x, d_y, d_z])\n",
    "\n",
    "print(f)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "[array(6.), array(3.), array(4.)]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np  # Thinly wrapped version of numpy\n",
    "from autograd import grad\n",
    "\n",
    "# define our top level function  f\n",
    "def f(args):\n",
    "    x,y,z = args\n",
    "    return (2*x + y)*z\n",
    "\n",
    "f_grad = grad(f)  # magic: returns a function that computes the gradient of f :) \n",
    "                  # This essentially just applies the chain rule to the function and\n",
    "                  # and takes the derivative, which is exactly what backprop is!\n",
    "\n",
    "# Plug in values for our inputs\n",
    "x = 1.\n",
    "y = 2.\n",
    "z = 3.\n",
    "\n",
    "print(f([x, y, z]))\n",
    "print(f_grad([x, y, z]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[6, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define our top level function  f\n",
    "def f(args):\n",
    "    x,y,z = args\n",
    "    return (2*x + y)*z\n",
    "\n",
    "# tensorflow uses it's own types of variables so it can return its matrices (tensors)\n",
    "# so we need to use tf variables\n",
    "x = tf.Variable(1.)\n",
    "y = tf.Variable(2.)\n",
    "z = tf.Variable(3.)\n",
    "\n",
    "# To run our gradient, we need to declare what is called a \"gradient tape\"\n",
    "# This is essentially a \"tape\" where tensorflow stores all of the intermediate derivatives\n",
    "# during backpropagation\n",
    "with tf.GradientTape() as tape:\n",
    "    y_hat = f([x, y, z])    # First we do our feedforward run to get our prediction from f\n",
    "\n",
    "grad = tape.gradient(y_hat, [x, y, z]) # Another magical gradient computation!\n",
    "\n",
    "tf.print(y_hat)\n",
    "tf.print(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Example 2\n",
    "\n",
    "Here is a slighly more complex example:\n",
    "\n",
    "$$f(x) = 10*\\exp(\\sin(x)) + \\cos^2(x)$$\n",
    "\n",
    "### Manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.1780070835713 11.92692295225547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Backprop example\n",
    "# f(x) = 10*np.exp(np.sin(x)) + np.cos(x)**2\n",
    "\n",
    "# Forward pass\n",
    "x = 1000\n",
    "a = np.sin(x)   # Node 1\n",
    "b = np.cos(x)   # Node 2\n",
    "c = b**2        # Node 3\n",
    "d = np.exp(a)   # Node 4\n",
    "f = 10*d + c    # Node 5 (final output)\n",
    "\n",
    "# Backward pass\n",
    "d_f = 1\n",
    "d_d = 10 * d_f            # Node 5 input\n",
    "d_c = 1  * d_f            # Node 5 input\n",
    "d_a = np.exp(a) * d_d     # Node 4 input\n",
    "d_b = 2*b * d_c           # Node 3 input\n",
    "d_x =  np.cos(x) * d_a - np.sin(x) * d_b  # Node 2 and 1 input\n",
    "\n",
    "print (f, x_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.1780070835713\n",
      "[11.92692295]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np  # Thinly wrapped version of numpy\n",
    "from autograd import grad\n",
    "\n",
    "# define our output function (result of forward pass)\n",
    "def f(args):\n",
    "    x = args\n",
    "    return 10*np.exp(np.sin(x)) + np.cos(x)**2\n",
    "\n",
    "# get the gradient function\n",
    "f_grad = grad(f)\n",
    "\n",
    "# set input value\n",
    "x = 1000\n",
    "\n",
    "print(f(x))\n",
    "print(f_grad([x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.178009]\n",
      "[11.9269238]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define our output function (result of forward pass)\n",
    "def f(args):\n",
    "    x = args\n",
    "    return 10*tf.exp(tf.sin(x)) + tf.cos(x)**2  # Use the tf version of exp, sin, and cos\n",
    "\n",
    "# tensorflow uses it's own types of variables so it can return its matrices (tensors)\n",
    "# so we need to use tf variables\n",
    "x = tf.Variable(1000.)\n",
    "\n",
    "# To run our gradient, we need to declare what is called a \"gradient tape\"\n",
    "# This is essentially a \"tape\" where tensorflow stores all of the intermediate derivatives\n",
    "# during backpropagation\n",
    "with tf.GradientTape() as tape:\n",
    "    y_hat = f([x])    # First we do our feedforward run to get our prediction from f\n",
    "\n",
    "grad = tape.gradient(y_hat, [x]) # Another magical gradient computation!\n",
    "\n",
    "tf.print(y_hat)\n",
    "tf.print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
