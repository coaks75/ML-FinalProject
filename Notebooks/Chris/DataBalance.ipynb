{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Imbalanced Data\n",
    "\n",
    "In machine learning, *imbalanced data* refers to training data where one class is vastly more common than the other. This can cause a problem, because we can create a learner that is very accurate (i.e., most of the predictions it makes are correct) because it simply always predicts the most common class, without really learning anything about the less common one. \n",
    "\n",
    "This is the case with our Kaggle data, where we have several thousands of examples of the 0 case (hospital not in distress) versus a couple hundred example of the 1 case (hospital in distress). A learner can perform very well (looks like about 95% correct predictions) just by predicting 0 every time and ignoring the 1 cases. It will get those wrong, but overall it is a lower proportion of the score, so it doesn't matter for the accuracy. Our AUC measure combats this slighly, as it is based on balancing the false negative rate as well, but we can still learn a better model if we have more balanced data. \n",
    "\n",
    "One way to do this is to artificially create a balanced dataset by \"oversampling\" the underrepresented class or by \"undersampling\" the very common class. This notebook will show two examples of doing this using scikit learn.\n",
    "\n",
    "**Note: You should always try to build a model with the original data distribution first, since this is the most accurate representation of how your testing data will look out in the wild.**\n",
    "\n",
    "To do this we will be using a new library called Imbalanced Learn that has a bunch of different data preprocessing tools that may be useful. You can get Imbalanced Learn by activating your anaconda environment and running:\n",
    "\n",
    "`conda install -c conda-forge imbalanced-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "data/X_train.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-50ed281b8970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# load in the data--I will do this with the Kaggle data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/X_train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Y_train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1770\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: data/X_train.txt not found."
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "from __future__ import division # For python 2.*\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import NearMiss \n",
    "\n",
    "\n",
    "# load in the data--I will do this with the Kaggle data\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=None) \n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=None) \n",
    "\n",
    "# remove the header rows; focus only on the classification value for Y (column 1)\n",
    "X,Y =(X[1:],Y[1:,1:])\n",
    "\n",
    "# Tells numpy not to print everything in scientific notation \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# print out the number of each class\n",
    "(y_class, counts) = np.unique(Y, return_counts=True)\n",
    "class_frequencies = np.asarray((y_class, counts)).T\n",
    "\n",
    "print(class_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, out of our dataset we have 4,542 0s and only 234 1s--I'd say this is pretty imbalanced! \n",
    "\n",
    "Let's learn a quick logistic regression model on this just to see how bad it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation data\n",
    "Xtr, Xval, Ytr, Yval = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# scale the data so our different X features and their ranges don't confuse the learner\n",
    "scale = StandardScaler().fit(Xtr)\n",
    "Xtr_scaled = scale.transform(Xtr)\n",
    "Xval_scaled = scale.transform(Xval)\n",
    "\n",
    "# logistic regression object \n",
    "lr = LogisticRegression() \n",
    "  \n",
    "# train the model on training data \n",
    "lr.fit(Xtr_scaled, Ytr.ravel()) \n",
    "\n",
    "# see how well we do on the training data\n",
    "pred_tr = lr.predict(Xtr_scaled)\n",
    "  \n",
    "# see how well we do on the validation data\n",
    "pred_val = lr.predict(Xval_scaled) \n",
    "  \n",
    "# print classification reports\n",
    "print(\"Performance on Training Data:\")\n",
    "print(classification_report(Ytr,pred_tr))\n",
    "\n",
    "print(\"\\n Performance on Validation Data:\")\n",
    "print(classification_report(Yval, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these prediction reports, the two things we want to look at are the precision (which is the prediction accuracy--of the predictions I made, how many were right?) and the recall (of the times the class should have been 1, how often was I right?). The learner currently has very, very high precision (overall it is 0.95 for the validation data), but very low recall for the class 1 because it is skewed toward predicting 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Oversampling\n",
    "\n",
    "The first method we will use for trying to balance out our data and get better at predicting the 1s is called oversampling. This is when we simply try and bump up the number of 1s relative to the number of 0s by either weighting the 1s higher or by physically resampling them to increase their numbers in the dataset. \n",
    "\n",
    "We are going to use a very popular resampling method called SMOTE (Synthetic Minority Oversampling Technique). SMOTE aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesizes new minority instances by creating new examples *between* existing minority instances. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class, learning a point in between them (i.e., a regression line), and generating a new point along that line. These new points are added to the training dataset, increasing the proportion of 1s represented.\n",
    "\n",
    "SMOTE is included in the Imbalanced Learn module, which you can get by running:\n",
    "\n",
    "`conda install -c conda-forge imbalanced-learn`\n",
    "\n",
    "in your Anaconda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 2) \n",
    "\n",
    "Xtr_res, Ytr_res = sm.fit_sample(Xtr, Ytr.ravel()) \n",
    "\n",
    "# print out the number of each class\n",
    "(y_class, counts) = np.unique(Ytr_res, return_counts=True)\n",
    "class_frequencies = np.asarray((y_class, counts)).T\n",
    "\n",
    "print(class_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an equal number of training examples for each possible class. This gives our learner more to work with in terms of understanding what is important to predicting distress. Let's retrain the learner and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data so our different X features and their ranges don't confuse the learner\n",
    "scale = StandardScaler().fit(Xtr_res)\n",
    "Xtr_res_scaled = scale.transform(Xtr_res)\n",
    "Xval_scaled = scale.transform(Xval)\n",
    "\n",
    "# logistic regression object \n",
    "lr_smote = LogisticRegression() \n",
    "  \n",
    "# train the model on training data \n",
    "lr_smote.fit(Xtr_res_scaled, Ytr_res.ravel()) \n",
    "\n",
    "# see how well we do on the training data\n",
    "pred_tr = lr_smote.predict(Xtr_res_scaled)\n",
    "  \n",
    "# see how well we do on the validation data\n",
    "pred_val = lr_smote.predict(Xval_scaled) \n",
    "  \n",
    "# print classification reports\n",
    "print(\"Performance on Training Data:\")\n",
    "print(classification_report(Ytr_res,pred_tr))\n",
    "\n",
    "print(\"\\n Performance on Validation Data:\")\n",
    "print(classification_report(Yval, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our accuracy is still pretty good (96% on the validation data!), but our recall on class 1 has gone way, way up. This is a much better model than before since it is actually predicting instances of distress.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Undersampling\n",
    "\n",
    "Another way we can do this preprocessing to rebalance the data is called undersampling, where we are reducing the prevalance of the more common class. For undersampling, we can use a technique called the NearMiss algorithm. NearMiss aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the separation between the two classes. To prevent problem of information loss in most under-sampling techniques, nearest-neighbor methods are widely used to average over the nearby majority instances.\n",
    "\n",
    "NearMiss is also available as part of the Imbalanced Learning package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NearMiss instance\n",
    "nr = NearMiss() \n",
    "\n",
    "# Apply NearMiss to our data\n",
    "Xtr_miss, Ytr_miss = nr.fit_sample(Xtr, Ytr.ravel()) \n",
    "\n",
    "# print out the number of each class\n",
    "(y_class, counts) = np.unique(Ytr_miss, return_counts=True)\n",
    "class_frequencies = np.asarray((y_class, counts)).T\n",
    "\n",
    "print(class_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a more balanced dataset, though we have made our data much smaller, which may lead to underfitting on the testing data. Let's see how it does when we build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data so our different X features and their ranges don't confuse the learner\n",
    "scale = StandardScaler().fit(Xtr_miss)\n",
    "Xtr_miss_scaled = scale.transform(Xtr_miss)\n",
    "Xval_scaled = scale.transform(Xval)\n",
    "\n",
    "# logistic regression object \n",
    "lr_miss = LogisticRegression() \n",
    "  \n",
    "# train the model on training data \n",
    "lr_miss.fit(Xtr_miss_scaled, Ytr_miss.ravel()) \n",
    "\n",
    "# see how well we do on the training data\n",
    "pred_tr = lr_miss.predict(Xtr_miss_scaled)\n",
    "  \n",
    "# see how well we do on the validation data\n",
    "pred_val = lr_miss.predict(Xval_scaled) \n",
    "  \n",
    "# print classification reports\n",
    "print(\"Performance on Training Data:\")\n",
    "print(classification_report(Ytr_miss,pred_tr))\n",
    "\n",
    "print(\"\\n Performance on Validation Data:\")\n",
    "print(classification_report(Yval, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also improved our recall on our data. Either of these might be useful preprocessing methods when trainig models for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
