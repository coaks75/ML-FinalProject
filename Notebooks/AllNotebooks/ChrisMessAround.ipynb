{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of mess around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt   # use matplotlib for plotting with inline plots\n",
    "plt.set_cmap('jet');\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore'); # for deprecated matplotlib functions\n",
    "\n",
    "import math\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import NearMiss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting imports\n",
    "In order to import mltools properly, we need to modify the module path that python thinks we have\n",
    "\n",
    "Basically it adds everything 2 folders up onto our python module path, so referencing \"mltools\" python can see that it is two directories up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need to append the module path so we can work with\n",
    "#     everything that is 2 folders up\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(os.getcwd())\n",
    "project_dir = str(path.parent.parent)\n",
    "\n",
    "picture_path = str(path) + os.path.sep + \"Pictures\" + os.path.sep\n",
    "model_path = str(path) + os.path.sep + \"Models\" + os.path.sep\n",
    "\n",
    "if not os.path.exists(picture_path):\n",
    "    os.mkdir(picture_path)\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "curr_path = sys.path\n",
    "data_path = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending' /home/chris/git/ML-FinalProject 'to path\n",
      "Appending' /home/chris/git/ML-FinalProject/data ' to path\n"
     ]
    }
   ],
   "source": [
    "if project_dir not in curr_path:\n",
    "    print(\"Appending'\", project_dir, \"'to path\")\n",
    "    sys.path.append(project_dir)\n",
    "    print(\"Appending'\", project_dir + os.path.sep + \"data '\", \"to path\")\n",
    "    sys.path.append(project_dir + os.path.sep + \"data\")\n",
    "    data_path = project_dir + os.path.sep + \"data\" + os.path.sep\n",
    "    \n",
    "# Uncomment this if youd like to see all your python module paths\n",
    "# print(sys.path)\n",
    "\n",
    "import mltools as ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Labels with Column index:\n",
      "\n",
      "0: UniqueID\n",
      "1: SIZE\n",
      "2: BIG4\n",
      "3: GAAP\n",
      "4: CR\n",
      "5: QR\n",
      "6: AT\n",
      "7: CashR\n",
      "8: NWC\n",
      "9: CFOCL\n",
      "10: OM\n",
      "11: TM\n",
      "12: CFM\n",
      "13: ROE\n",
      "14: ROA\n",
      "15: GRIE\n",
      "16: CFONI\n",
      "17: CFOTL\n",
      "18: CFOTR\n",
      "19: CFOTA\n",
      "20: EF\n",
      "21: FAF\n",
      "22: LTDE\n",
      "23: CAT\n",
      "24: DCOH\n",
      "25: ACP\n",
      "26: AAP\n",
      "27: DR\n",
      "28: DSCR\n",
      "29: CFOLTL\n",
      "30: TAT\n",
      "31: FAT\n",
      "32: DEPR\n",
      "33: CFOFA\n",
      "34: APP\n",
      "35: 2008\n",
      "36: 2009\n",
      "37: 2010\n",
      "38: 2011\n",
      "39: 2012\n",
      "40: 2013\n",
      "41: 2014\n",
      "42: 2015\n",
      "43: 2016\n",
      "44: 2017\n",
      "45: 2018\n",
      "Class Frequencies:\n",
      " [[   0. 4542.]\n",
      " [   1.  233.]]\n",
      "Bias: [-2.96880144]\n"
     ]
    }
   ],
   "source": [
    "# print(sys.path)\n",
    "# print(data_path)\n",
    "X_train = np.genfromtxt(data_path + \"X_train.txt\", delimiter=None, skip_header=1)\n",
    "Xtest = np.genfromtxt(data_path + \"X_test.txt\", delimiter=None, skip_header=1)\n",
    "Y_train = np.genfromtxt(data_path + \"Y_train.txt\", delimiter=None, skip_header=1)\n",
    "Data_Labels = np.genfromtxt(data_path + \"X_train.txt\", delimiter=None, dtype=str, max_rows=1)\n",
    "\n",
    "Xtrain,Xval,Ytrain,Yval = ml.splitData(X_train,Y_train[:, 1], 0.75) # split data set 75/25\n",
    "\n",
    "print(\"Data Labels with Column index:\\n\")\n",
    "for i in range(len(Data_Labels)):\n",
    "    print(str(i) + \": \" + str(Data_Labels[i]))\n",
    "\n",
    "X,Y =(X_train[1:],Y_train[1:,1:])\n",
    "\n",
    "# Tells numpy not to print everything in scientific notation \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# print out the number of each class\n",
    "(y_class, counts) = np.unique(Y, return_counts=True)\n",
    "class_frequencies = np.asarray((y_class, counts)).T\n",
    "\n",
    "print(\"Class Frequencies:\\n\", class_frequencies)\n",
    "\n",
    "tot_dist = 0\n",
    "for i in range(len(Ytrain)):\n",
    "    if Ytrain[i] == 1:\n",
    "        tot_dist += 1\n",
    "\n",
    "neg = Ytrain.shape[0] - tot_dist\n",
    "initial_bias = np.log([tot_dist/neg])\n",
    "\n",
    "print(\"Bias:\", initial_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 8 # 1st layer number of neurons\n",
    "n_hidden_2 = 16 # 2nd layer number of neurons\n",
    "n_hidden_3 = 16\n",
    "n_hidden_4 = 8\n",
    "        \n",
    "# Parameters\n",
    "learning_rate = 0.01    # alpha for gradient descent\n",
    "num_steps = 4000       # iterations for gradient descent\n",
    "batch_size = 128       # number of inputs to look at simultaneously (good for large data!)\n",
    "display_step = 100     # when to print out some feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our NN model, this inherits from TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Model. Our NeuralNet inherits from the generic TF Model class\n",
    "class NeuralNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self, num_classes, output_bias):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        curr_activation = tf.nn.relu\n",
    "        # First fully-connected hidden layer. Activation function (threshold function) is \n",
    "        # rectified linear.\n",
    "#         output_bias=tf.constant_initializer(output_bias[0])\n",
    "        output_bias = tf.zeros_initializer()\n",
    "#         output_bias=tf.keras.backend.variable(output_bias[0])\n",
    "        self.h1 = layers.Dense(n_hidden_1, activation=curr_activation)\n",
    "        \n",
    "        # First fully-connected hidden layer. Activation function (threshold function) is \n",
    "        # rectified linear.\n",
    "        self.h2 = layers.Dense(n_hidden_2, activation=curr_activation)\n",
    "        self.h3 = layers.Dense(n_hidden_3, activation=curr_activation)\n",
    "        self.h4 = layers.Dense(n_hidden_4, activation=curr_activation)\n",
    "        \n",
    "        # Second fully-connecter hidden layer. Activation function is using \"softmax\" to\n",
    "        # normalize output as a probability distribution over the 2 classes (digits 0 or 1)\n",
    "        self.out = layers.Dense(num_classes, activation=tf.nn.softmax)\n",
    "\n",
    "    # Set forward pass--this defines the input layer (h1) and output layer (out) and then\n",
    "    # formats the final results as a probability distribution over the classes using softmax\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.h1(x)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits (positions on the logistic regression sigmoid)\n",
    "            # without softmax normalization, so only apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits as part of the function, so don't do\n",
    "# it before calling.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric. This counts how many of our predictions we get right based on \n",
    "# choosing the prediction with the highest probability. \n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y, nn, optimizer):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation \n",
    "    # (see Backprop.ipnb for an explanation of the GradientTape)\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = nn(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = nn.trainable_variables\n",
    "\n",
    "    # Compute gradients (backpropagation).\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update all of the weights W and biases (y-intercepts) b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "def run_training(nn, training_data, optimizer):\n",
    "    # Run training for the given number of steps.\n",
    "    for step, (batch_x, batch_y) in enumerate(training_data.take(num_steps), 1):\n",
    "        # Run the optimization to update W and b values.\n",
    "        run_optimization(batch_x, batch_y, nn, optimizer)\n",
    "\n",
    "        if step % display_step == 0 or step==1:\n",
    "            pred = nn(batch_x, is_training=True)     \n",
    "            loss = cross_entropy_loss(pred, batch_y)\n",
    "            acc = accuracy(pred, batch_y)\n",
    "#             print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat1 = np.linspace(4, 9, 9-4+1, dtype=int)\n",
    "# cat2 = np.linspace(10, 19, 19-10+1, dtype=int)\n",
    "# cat3 = np.linspace(20, 22, 22-20+1, dtype=int)\n",
    "# cat4 = np.linspace(23, 26, 26-23+1, dtype=int)\n",
    "# cat5 = np.linspace(27, 29, 29-27+1, dtype=int)\n",
    "# cat6 = np.linspace(30, 33, 33-30+1, dtype=int)\n",
    "# cat7 = np.linspace(34, 34, 1, dtype=int)\n",
    "\n",
    "cat1 = [6, 7, 4, 8, 5]\n",
    "cat2 = [17, 14, 11]\n",
    "cat3 = [20, 21]\n",
    "cat4 = [24]\n",
    "cat5 = [27]\n",
    "cat6 = [30, 31]\n",
    "cat7 = [34]\n",
    "\n",
    "all_cats = [cat1, cat2, cat3, cat4, cat5, cat6, cat7]\n",
    "\n",
    "for x in range(len(all_cats)):\n",
    "    new_cat = np.append(all_cats[x], -1)\n",
    "    all_cats[x] = new_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def test_nn(cols_using, bias):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    X_train, X_val = np.array(Xtrain[:, cols_using], np.float32), np.array(Xval[:, cols_using], np.float32)\n",
    "    X_test = np.array(Xtest[:, cols_using], np.float32)\n",
    "    \n",
    "    sm = SMOTE(random_state = 2)\n",
    "    \n",
    "    Xtr_res, Ytr_res = sm.fit_sample(X_train, Ytrain.ravel())\n",
    "    \n",
    "    scale = StandardScaler().fit(Xtr_res)\n",
    "    Xtr_res_scaled = scale.transform(Xtr_res)\n",
    "    Xval_scaled = scale.transform(X_val)\n",
    "        \n",
    "    # Use tf.data API to shuffle and batch data (batches will make it faster to queue up \n",
    "    # sets of images for training all at one time--a convenient way to split up large data sets)\n",
    "    oversample_data = tf.data.Dataset.from_tensor_slices((Xtr_res_scaled, Ytr_res.ravel()))\n",
    "\n",
    "    oversample_data = oversample_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
    "    \n",
    "#     print(\"Columns we are currently using:\", Data_Labels[cols_using])\n",
    "\n",
    "    # Data Parameters\n",
    "    num_features = len(cols_using)\n",
    "    num_classes = 2 # We have 2 classes, yes or no\n",
    "\n",
    "    # Build neural network model.\n",
    "    neural_net_over_sample = NeuralNet( num_classes=num_classes, output_bias=bias)\n",
    "\n",
    "    # Stochastic gradient descent optimizer.\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "    run_training(neural_net_over_sample, oversample_data, optimizer)\n",
    "    \n",
    "    # Test model on validation set.\n",
    "    pred = neural_net_over_sample(Xval_scaled, is_training=False)\n",
    "    curr_acc = accuracy(pred, Yval)\n",
    "#     print(\"Test Accuracy: %f\" % accuracy(pred, Yval))\n",
    "#     print()\n",
    "    \n",
    "    return float(curr_acc), scale, neural_net_over_sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a new best accuracy: 0.7546063661575317\n",
      "On test number: 2\n",
      "Used columns: [1, 2, 3, 6, 17, 20, 25, 27, 34]\n",
      "\n",
      "Found a new best accuracy: 0.7604690194129944\n",
      "On test number: 3\n",
      "Used columns: [1, 2, 3, 6, 14, 20, 25, 27, 34]\n",
      "\n",
      "Found a new best accuracy: 0.876884400844574\n",
      "On test number: 4\n",
      "Used columns: [1, 2, 3, 6, 11, 20, 25, 27, 34]\n",
      "\n",
      "Found a new best accuracy: 0.8835846185684204\n",
      "On test number: 8\n",
      "Used columns: [1, 2, 3, 7, 11, 20, 25, 27, 34]\n",
      "\n",
      "Found a new best accuracy: 0.8886097073554993\n",
      "On test number: 12\n",
      "Used columns: [1, 2, 3, 4, 11, 20, 25, 27, 34]\n",
      "\n",
      "Found a new best accuracy: 0.8978224396705627\n",
      "On test number: 16\n",
      "Used columns: [1, 2, 3, 8, 11, 20, 25, 27, 34]\n",
      "\n",
      "Best accuracy was: 0\n",
      "It used the columgs: [1, 2, 3, 8, 11, 20, 25, 27, 34]\n"
     ]
    }
   ],
   "source": [
    "def test_options():\n",
    "    best_accuracy = 0.0\n",
    "    best_cols = []\n",
    "    test_num = 1\n",
    "\n",
    "    for a in all_cats[0]:\n",
    "        for b in all_cats[1]:\n",
    "    #         for c in all_cats[2]:\n",
    "    #             for d in all_cats[3]:\n",
    "    #                 for e in all_cats[4]:\n",
    "    #                     for f in all_cats[5]:\n",
    "    #                         for g in all_cats[6]:\n",
    "            cols_using = [1, 2, 3]\n",
    "            if a != -1:\n",
    "                cols_using.append(a)\n",
    "            if b != -1:\n",
    "                cols_using.append(b)\n",
    "            cols_using.append(20)\n",
    "            cols_using.append(25)\n",
    "            cols_using.append(27)\n",
    "            cols_using.append(34)\n",
    "    #                             if c != -1:\n",
    "    #                                 cols_using.append(c)\n",
    "    #                             if d != -1:\n",
    "    #                                 cols_using.append(d)\n",
    "    #                             if e != -1:\n",
    "    #                                 cols_using.append(e)\n",
    "    #                             if f != -1:\n",
    "    #                                 cols_using.append(f)\n",
    "    #                             if g != -1:\n",
    "    #                                 cols_using.append(g)\n",
    "\n",
    "            test_num += 1\n",
    "            acc = test_nn(cols_using, initial_bias)[0]\n",
    "\n",
    "            if (acc > best_accuracy):\n",
    "                best_accuracy = acc\n",
    "                best_cols = cols_using\n",
    "                print(\"Found a new best accuracy:\", best_accuracy)\n",
    "                print(\"On test number:\", test_num)\n",
    "                print(\"Used columns:\", best_cols)\n",
    "                print()\n",
    "\n",
    "\n",
    "    print(\"Best accuracy was: %i\" % best_accuracy)\n",
    "    print(\"It used the columgs:\", best_cols)\n",
    "\n",
    "test_options()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.905360\n",
      "Num in distres: 3\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "submission_path = model_path + str(st) + os.path.sep\n",
    "os.mkdir(submission_path)\n",
    "\n",
    "temp_cols = [1, 2, 3, 8, 11, 20, 25, 27, 34]\n",
    "acc, scaler, nn = test_nn(temp_cols, initial_bias)\n",
    "\n",
    "XTrain, XVal = np.array(Xtrain[:, temp_cols], np.float32), np.array(Xval[:, temp_cols], np.float32)\n",
    "XTest = np.array(Xtest[:, temp_cols], np.float32)\n",
    "\n",
    "Xval_scaled = scaler.transform(XVal)\n",
    "Xtest_scaled = scaler.transform(XTest)\n",
    "\n",
    "# Test model on validation set.\n",
    "pred = nn(Xval_scaled, is_training=False)\n",
    "# pred = nn.predict(Xval_scaled)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, Yval))\n",
    "pred2 = nn(Xtest_scaled, is_training=False)\n",
    "\n",
    "file = open(submission_path + \"submission.txt\",\"w+\")\n",
    "file.write(\"Unique ID,Dist\\n\")\n",
    "num_distress = 0\n",
    "num_dist = 0\n",
    "for i in range(len(Xtest_scaled)):\n",
    "    prediction = np.argmax(pred2.numpy()[i])\n",
    "    file.write(str(int(Xtest[i, 0])) + \",\" + str(pred2.numpy()[i][1]) + \"\\n\")\n",
    "    num_distress += prediction\n",
    "\n",
    "file.close()\n",
    "\n",
    "# nn.save(submission_path + \"model\")\n",
    "\n",
    "# n.save_model(submission_path + \"model\")\n",
    "\n",
    "print(\"Num in distres: %i\" %num_distress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
